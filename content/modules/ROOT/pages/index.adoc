= InstructLab ワークショップ: Parasol Insurance Company 向けのカスタム LLM のトレーニング

:experimental: true
:imagesdir: ../assets/images
:toc: false
:numbered: true

== イントロダクション

InstructLab について学び、使用するために時間を割いていただきありがとうございます。このハンズオンラボでは、InstructLab とは何かと、そしてそれを使用して会社に価値を提供する方法を学びます。InstructLab は https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/ai[RHEL AI] のコアコンポーネントであり、大規模言語モデル (LLM) に貢献および改善するために使用できます。

RHEL AI はいくつかのコアコンポーネントで構成されています。

. https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/image-mode[RHEL Image Mode]
. https://www.ibm.com/granite[Granite] Large Language Model(s)（大規模言語モデル）
. モデルアライメントのための https://www.redhat.com/en/topics/ai/what-is-instructlab[InstructLab]
. Red Hat からのサポートと補償

https://www.redhat.com/en/topics/ai/what-is-instructlab[InstructLab] は、Red Hat と MIT-IBM Watson AI Lab による完全なオープンソース プロジェクトです。MIT-IBM Watson AI Labは https://arxiv.org/abs/2403.01081[Large-scale Alignment for chatBots] (LAB)（チャットボットの大規模調整）を発表しました。このプロジェクトのイノベーションは、LLM トレーニングの指導調整段階で役立ちます。ただし、このプロジェクトの利点を完全に理解するには、LLM とは何か、モデルのトレーニングに関連する難しさとコストについて、いくつかの基本概念を理解しておく必要があります。

[#llms]
=== 大規模言語モデルとは何でしょうか？

大規模言語モデル (large language model: LLM) は、深層学習技術 (deep learning techniques) を使用して入力データに基づいて人間のようなテキストを理解して生成する人工知能 (artificial intelligence: AI) モデルの一種です。これらのモデルは、膨大な量のテキスト データを分析し、データ内のパターン、関係、構造を学習するように設計されています。これらは、次のようなさまざまな自然言語処理 (natural language processing: NLP) タスクに使用できます。

* *Text classification*: スパム検出や感情分析など、内容に基づいてテキストを分類します。
* *Text summarization*: ニュース記事や研究論文など、長いテキストの簡潔な要約を生成します。
* *Machine translation*: 英語からフランス語、ドイツ語から中国語など、ある言語から別の言語へのテキストの翻訳します。
* *Question answering*: 特定のコンテキストまたは一連の文書に基づいて質問に回答します。
* *Text generation*: 記事、物語、さらには詩を書くなど、一貫性があり、文脈に関連性があり、文法的に正しい新しいテキストを作成します。

大規模な言語モデルには通常、データ内の複雑な言語パターンと関係を捉えることを可能にする多くのパラメーター (数百万から数十億) があります。これらは、教師なし事前トレーニングや教師あり微調整などの手法を使用して、書籍、記事、Web サイトなどの大規模なデータセットでトレーニングされます。人気のある大規模言語モデルには、GPT-4、Llama、Mistral などがあります。

要約すると、大規模言語モデル (LLM) は、深層学習技術を使用して、入力データに基づいて人間のようなテキストを理解して生成する人工知能モデルです。これらは、膨大な量のテキスト データを分析し、データ内のパターン、関係、構造を学習するように設計されており、さまざまな自然言語処理タスクに使用できます。

NOTE: LLM が何を達成できるかを理解していただくために、このセクション全体は、ワークショップで使用している基本モデルに対する簡単な質問で生成しました。

[#how_trained]
=== 大規模言語モデルはどのようにトレーニングされるのでしょうか？

大規模言語モデル (LLM) は通常、深層学習技術と大規模なデータセットを使用してトレーニングされます。トレーニングプロセスにはいくつかのステップが含まれます。

. *Data Collection*: 膨大な量のテキスト データが書籍、記事、Web サイト、データベースなどのさまざまなソースから収集されます。モデルを適切に一般化できるように、データにはさまざまな言語、ドメイン、スタイルが含まれる場合があります。
. *Pre-processing*: 生のテキスト データは前処理され、ノイズ、不一致、無関係な情報が除去されます。これには、トークン化、小文字化、ステミング、見出し語化、およびエンコードが含まれる場合があります。
. *Tokenization*: 前処理されたテキスト データは、モデルへの入力および出力として使用できるトークン (単語またはサブワード) に変換されます。一部のモデルでは、バイト ペア エンコーディング (BPE) またはサブワード セグメンテーションを使用して、語彙外の単語を処理し、コンテキスト情報を維持できるトークンを作成します。
. *Pre-training*: モデルは教師なしまたは自己教師ありの方法でトレーニングされ、データ内のパターンと構造を学習します。
. *Model Alignment*: (命令チューニングとプリファレンスチューニング): 人間の価値観と目標を大規模な言語モデルにエンコードして、可能な限り役立つ、安全、信頼できるものにするプロセス。このステップは、他の一部のステップほど計算集約的ではありません。

[#instructlab]
=== それは InstructLab とどのように関係するのでしょうか？

InstructLab は、分類学に基づいた合成データ生成プロセスとマルチフェーズ チューニングフレームワークを活用しています。これにより、InstructLab は高価な人間による注釈への依存を大幅に削減し、大規模な言語モデルへの貢献が簡単かつアクセスしやすくなります。これは、InstructLab が LLM を使用してデータを生成し、そのデータを使用して LLM をさらにトレーニングできることを意味します。これは、調整フェーズがほとんどのユーザーにとって知識を提供するための出発点になることも意味します。  LAB 手法が導入される前は、通常、ユーザーは LLM のトレーニングに直接関与していませんでした。複雑に聞こえるかもしれませんが、ちょっと待ってください、これからこれがいかに使いやすいかがわかるでしょう。

InstructLab を使用していると、スキルと知識という用語が表示されます。スキルと知識の違いは何ですか?簡単な例えとしては、スキルを誰かに釣り方を教えることだと考えてください。一方、知識とは、バスを釣るのに、最適な場所は日が沈む時間帯で、岸沿いの木の幹の近くにラインをキャストすることを知っていることです。

[#getting_started]
== InstructLab を始める

[#installation]
=== ilab コマンドラインインターフェースのインストール

私たちは、ローカル LLM 開発者エクスペリエンスとワークフローを実装する https://github.com/instructlab/instructlab[ilab] という CLI ツールを作成しました。 ilab CLI は Python で書かれており、次のアーキテクチャで動作します。

. Apple M1/M2/M3 Mac
. Linux システム
. Windows 11 の WSL 環境

このラボでは、Red Hat Enterprise Linux システム上で ilab コマンド ライン ツールを使用します。将来的には、さらに多くのオペレーティング システムがサポートされる予定です。コマンド ライン ツールを使用するためのシステム要件は次のとおりです。

. C++ コンパイラー
. Python 3.10 または Python 3.11
. 約 60GB のディスクスペース (全ての手順を実行する場合)
.. ディスク容量の要件は、いくつかの要因によって異なります。モデルをシステム上にローカルに置きながら、モデルにフィードするデータを生成することに注意してください。たとえば、このワークショップで使用しているモデルのサイズは約 5 GB です。

[#configuration]
=== ilab の実行

最初に行う必要があるのは、InstructLab コマンド ライン ツールと対話できるようにする Python 仮想環境を調達することです。右側に 2 つのターミナル ウィンドウが表示されますが、*top* ウィンドウから始めましょう。

. 事前設定されている InstructLab フォルダーに移動し、次のコマンドを実行して Python 仮想環境をアクティブ化します。
+

[source,console,role=execute,subs=attributes+]
----
cd ~/instructlab
source venv/bin/activate
----
+
.プロンプトは次のようになります。

[source,console]
----
(venv) [instruct@bastion instructlab]$
----
+

[start=2]
. InstructLab はすでにプリインストールされています。 venv 環境から ilab コマンドを実行して、ilab が正しくインストールされていることを確認します。
+

[source,console,role=execute,subs=attributes+]
----
ilab
----
+

すべてが正しくインストールされていると仮定すると、次の出力が表示されるはずです。
+

[subs:quotes]
----
Usage: ilab [OPTIONS] COMMAND [ARGS]...


  CLI for interacting with InstructLab.


  If this is your first time running ilab, it's best to start with `ilab config init`
  to create the environment.


Options:
  --config PATH  Path to a configuration file.  [default: /home/instruct/.config/instructlab/config.yaml]
  -v, --verbose  Enable debug logging (repeat for even more verbosity)
  --version      Show the version and exit.
  --help         Show this message and exit.

Commands:
  config    Command Group for Interacting with...
  data      Command Group for Interacting with...
  model     Command Group for Interacting with...
  system    Command group for all system-related...
  taxonomy  Command Group for Interacting with...

Aliases:
  chat      model chat
  generate  data generate
  serve     model serve
  train     model train
----


*CONGRATULATIONS!* これですべてがインストールされ、LLM アライメントの世界に飛び込む準備が整いました。

[#initialize]
=== ilabの初期化

コマンドライン インターフェイス `ilab` が正しく動作していることがわかったので、次に行う必要があるのは、モデルの操作を開始できるようにローカル環境を初期化することです。これは、単純な init コマンドを発行することで実現されます。次のコマンドを実行して `ilab` を初期化します。:

[source,console,role=execute,subs=attributes+]
----
ilab config init
----

次の出力が表示されるはずです。（ kbd:[ENTER] でデフォルトを選択）

[subs:quotes]
----
Welcome to InstructLab CLI. This guide will help you to setup your environment.
Please provide the following values to initiate the environment [press Enter for defaults]:
Path to taxonomy repo [/home/instruct/.local/share/instructlab/taxonomy]:
----

NOTE:  kbd:[ENTER] を全てデフォルト設定で問題ありません。

[subs:quotes]
----
Path to your model [/home/instruct/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf]:
Generating `/home/instruct/.config/instructlab/config.yaml`...
Detecting Hardware...
We chose Nvidia 1x L4 as your designated training profile. This is for systems with 24 GB of vRAM.
This profile is the best approximation for your system based off of the amount of vRAM. We modified it to match the number of GPUs you have.
Is this profile correct? [Y/n]: Y
----

上に示したように「Y」を入力するか、kbd:[ENTER] を押してトレーニング プロファイル設定を受け入れます。 **このラボでは** 、上記の出力で説明されているように、１枚の NVIDIA L4 GPU を使用しています。

[subs:quotes]
----
Initialization completed successfully, you're ready to start using `ilab`. Enjoy!
----

* 初期化フェーズではいくつかの処理が行われます。デフォルトの分類法がローカル ファイル システムに配置され、構成ファイル (config.yaml) が現在のディレクトリに作成されます。
* config.yaml ファイルには、このワークショップ中に使用するデフォルトが含まれています。このワークショップの後、InstructLab を試し始めるときは、パラメーターを好みに合わせて調整できるように、構成ファイルの内容を理解することが重要です。


[#download]
=== モデルのダウンロード

InstructLab 環境を構成したら、2 つの異なる量子化 (圧縮および最適化) モデルをローカル ディレクトリにダウンロードします。 Granite は API リクエストのモデル サーバーとして使用され、Merlinite は新しいモデルをトレーニングするための合成データの作成に役立ちます。

NOTE: このラボでは 1 つの GPU のみを利用しているため、量子化モデルを使用しています。パフォーマンスや運用環境を向上させるには、量子化されていないモデルを使用します。

以下に示すように `ilab model download` コマンドを実行します。

まず、Granite をダウンロードしましょう。

[source,console,role=execute,subs=attributes+]
----
ilab model download --repository instructlab/granite-7b-lab-GGUF --filename=granite-7b-lab-Q4_K_M.gguf
----

One more time, let's pull down Merlinite:

[source,console,role=execute,subs=attributes+]
----
ilab model download --repository instructlab/merlinite-7b-lab-GGUF --filename=merlinite-7b-lab-Q4_K_M.gguf
----

`ilab model download` コマンドは、このワークショップで使用するモデルを HuggingFaceのinstructlab 組織からダウンロードします。出力は次のようになります。

[subs:quotes]
----
Downloading model from Hugging Face: instructlab/granite-7b-lab-GGUF@main to /home/instruct/.cache/instructlab/models...
Downloading 'granite-7b-lab-Q4_K_M.gguf' to '/home/instruct/.cache/instructlab/models/.cache/huggingface/download/granite-7b-lab-Q4_K_M.gguf.6adeaad8c048b35ea54562c55e454cc32c63118a32c7b8152cf706b290611487.incomplete'
INFO 2024-09-10 16:51:32,740 huggingface_hub.file_download:1908: Downloading 'granite-7b-lab-Q4_K_M.gguf' to '/home/instruct/.cache/instructlab/models/.cache/huggingface/download/granite-7b-lab-Q4_K_M.gguf.6adeaad8c048b35ea54562c55e454cc32c63118a32c7b8152cf706b290611487.incomplete'
granite-7b-lab-Q4_K_M.gguf: 100%|█| 4.08G/4.08G [00:19<00:00, 207
Download complete. Moving file to /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
INFO 2024-09-10 16:51:52,562 huggingface_hub.file_download:1924: Download complete. Moving file to /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----

モデルがダウンロードされたので、モデルを提供してチャットできるようになります。モデルを提供するということは、他のプログラムが API 呼び出しを行うのと同様にデータを操作できるようにするサーバーを実行することを意味します。

[#serve]
=== モデルの提供

次のコマンドを実行してモデルを提供しましょう。

[source,console,role=execute,subs=attributes+]
----
ilab model serve --model-path /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----

ご覧のとおり、serve コマンドはオプションの `-–model-path` 引数を取ることができます。この場合、Granite モデルを提供したいと考えています。モデル パスが指定されていない場合は、config.yaml ファイルのデフォルト値が使用されます。

モデルが提供されて準備が完了すると、次の出力が表示されます。

[subs:quotes]
----
INFO 2024-09-10 18:12:09,459 instructlab.model.serve:145: Using model '/home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size.
INFO 2024-09-10 18:12:09,459 instructlab.model.serve:149: Serving model '/home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf' with llama-cpp
INFO 2024-09-10 18:12:16,023 instructlab.model.backends.llama_cpp:250: Replacing chat template:
 {% for message in messages %}
{% if message['role'] == 'user' %}
{{ '<|user|>
' + message['content'] }}
{% elif message['role'] == 'system' %}
{{ '<|system|>
' + message['content'] }}
{% elif message['role'] == 'assistant' %}
{{ '<|assistant|>
' + message['content'] + eos_token }}
{% endif %}
{% if loop.last and add_generation_prompt %}
{{ '<|assistant|>' }}
{% endif %}
{% endfor %}
INFO 2024-09-10 18:12:16,026 instructlab.model.backends.llama_cpp:193: Starting server process, press CTRL+C to shutdown server...
INFO 2024-09-10 18:12:16,026 instructlab.model.backends.llama_cpp:194: After application startup complete see http://127.0.0.1:8000/docs for API.

----

*WOOHOO!* 初めてモデルを提供したばかりで、LLM と対話してこれまでの作業をテストする準備ができています。これから、モデルとチャットすることで実現します。


[#chat]
=== モデルに対してチャットする

1 つのターミナル ウィンドウでモデルを提供しているため、 `ilab chat` コマンドを実行して提供しているモデルと通信するには、別のターミナル ウィンドウを使用し、Python 仮想環境を再アクティブ化する必要があります。

.  *bottom* ターミナル ウィンドウで、次のコマンドを発行します。

[source,console,role=execute,subs=attributes+]
----
cd ~/instructlab
source venv/bin/activate
----

.プロンプトは次のようになります。
[source,console]
----
(venv) [instruct@bastion instructlab]$
----

[start=2]
. 環境が提供されたので、`ilab model chat` コマンドを使用してチャット セッションを開始できます。

[source,console,role=execute,subs=attributes+]
----
ilab model chat -m /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----

以下の例のようなチャット プロンプトが表示されるはずです。

[subs:quotes]
----
╭──────────────────────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ GRANITE-7B-LAB-Q4_K_M.GGUF (type /h for help) │
╰──────────────────────────────────────────────────────────────────────────────╯
>>>
----

[start=3]
. この時点で、モデルに質問することでモデルと対話できるようになります。
例：What is openshift in 20 words or less?（openshift を 20 語以内で説明してください。）
（現状ilabは日本語の入力をサポートしていないので英語での入力をお勧めします。）

[source,console,role=execute,subs=attributes+]
----
What is OpenShift in 20 words or less?
----

待って、あれ？最高!!!!!これで、このマシン上で独自のローカル LLM が実行されました。とても簡単でしたね。

[#integrating_instructlab]
== AI を保険アプリケーションに統合する

前のセクションでは、InstructLab と対話する方法の基本を説明しました。次に、サンプル アプリケーションで InstructLab を使用して、さらに一歩進んでみましょう。 RHEL AI を使用して granite LLM を活用し、知識やスキルの形でデータを追加し、新しい知識でモデルをトレーニングし、質問に効果的に回答できるようにします。これは、保険請求（insurance claims）を処理する架空の会社である Parasol のコンテキストで行われます。

Parasol には、AI (graniteモデル) を組み込んだチャットボット アプリケーションがあり、提出された請求に対して修理提案を提供します。これにより、Parasol は保留中のさまざまな請求の処理を迅速化できるようになります。しかし現時点では、チャットボットは効果的な修理提案を提供しません。さまざまな条件下で実行されたさまざまな修理を含む過去の請求データを使用して、ユーザーがこの知識をgraniteモデルに追加し、追加の知識に基づいてトレーニングし、推奨事項を改善する方法を示します。

[#using_parasol_application]
=== Parasol アプリケーションの使用

まず、請求担当者がチャットボットと対話する際の現在のエクスペリエンスを見てみましょう。

. 現在 *Terminals* ビューにいる場合、*Parasol* (上部ターミナル ウィンドウの上の上部バー) に切り替えて、ブラウザーに Parasol のクレーム アプリケーションを表示します。

image::parasol-view.png[]

請求担当者は、画面上の請求番号をクリックすると、既存の請求に移動して表示できます。

[start=2]
. このラボでは、Marty McFly によって提出された請求である *CLM195501* を調査します。この請求をクリックしてみましょう。

image::parasol-claim.png[]

このページでは請求の詳細を見ることができます。

[start=3]
. 請求の内容を読んだら、ページの右下にある小さな青いアイコンを使用してチャットボットをクリックします。

image::parasol-chat.webp[width=350]

IMPORTANT: このチャットボットは、以前に提供した Granite モデルによってサポートされているため、実行中のプロセスを強制終了した場合は、次のコマンドを実行してターミナルで再起動する必要があります。 `ilab model serve --model-path /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf`

あなたが保険請求担当者として、マーティのデロリアンの磁束コンデンサの修理にどれくらいの費用がかかるかを知りたいと考えていると想像してください。 

[start=4]
. チャットボットに次の質問をしてください。

[source,console,role=execute,subs=attributes+]
----
How much does it cost to repair a flux capacitor?
----


次のような内容が表示されるはずです。 LLM は本質的に非決定的であることに注意してください。これは、同じプロンプト入力であっても、モデルがさまざまな応答を生成することを意味します。したがって、結果は若干異なる場合があります。

image::parasol-chat-response.webp[width=350]

私たちがすでに始めていることは、プロンプト エンジニアリングを使用して、LLM との各会話で請求に関するコンテキスト情報を提供することです。しかし、残念ながら、チャットボットは磁束コンデンサの修理にどれくらいの費用がかかるのか、または私たちの組織のドメイン固有の知識を知りません。 

InstructLab と RHEL AI を使用すると、モデルを教えることでこの状況を変えることができます。

[#taxonomy]
=== 分類法（taxonomy）の理解

InstructLab は、大規模言語モデル (LLM) に新しい合成データによるアライメント調整手法を使用します。InstructLab の「lab」は、**L**arge-scale **A**lignment for Chat **B**ots を表します。 

LAB メソッドは分類法によって駆動され、分類法は主に手作業で慎重に作成されます。

InstructLab は、新しい InstructLab オープンソース コミュニティで知識とスキルという 2 種類のデータを収集することで、モデルのチューニングと改善のプロセスをクラウドソーシングします。それらの提供物は、分類されたYAML ファイルとして収集され、合成データ生成プロセスで使用されます。分類のディレクトリ構造を理解するには、次の画像を参照してください。


image::taxonomy.png[]

これから、分類モデルを活用して、組織の公開 (および非公開) 内部データのコレクションから、対象となる特定の車両とその詳細に関する知識をモデルに教えます。

*Terminals* ビューに戻り、チャットを実行しているターミナル ウィンドウで、「exit」と入力してチャット セッションを終了します。

. taxonomy ディレクトリに移動します。

[source,console,role=execute,subs=attributes+]
----
cd /home/instruct/.local/share/instructlab
tree taxonomy | head -n 10
----

.以下に示すように分類ディレクトリがリストされているはずです。
[source,texinfo]
----
taxonomy
├── CODE_OF_CONDUCT.md
├── compositional_skills
│   ├── arts
│   ├── engineering
│   ├── geography
│   ├── grounded
│   │   ├── arts
│   │   ├── engineering
│   │   ├── geography
----

次に、ファイルを配置できるディレクトリを作成する必要があります。

[start=2]
. 新しい知識を追加するためのディレクトリを作成し、分類構造を適切に使用して InstructLab で知識を追加する方法を示します。

[source,console,role=execute,subs=attributes+]
----
mkdir -p /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims
----

[start=3]
. 新しい知識を通じてモデルに新しい機能を追加します。 

分類アプローチが機能する方法は、質問と回答のサンプル データ セットを含む `qna.yaml` という名前のファイルを提供することです。このデータ セットは、モデルの出力に完全に影響を与えるのに十分な、さらに多くの合成データ サンプルを作成するプロセスで使用されます。  `qna.yaml` ファイルについて理解する重要な点は、InstructLab がそれを使用してより多くの例を合成的に生成するには、このファイルが特定のスキーマに従う必要があるということです。

`qna.yaml` ファイルは、分類ディレクトリの `knowledge` サブディレクトリ内のフォルダーに配置されます。以下のコマンドでわかるように、データ トピックに合わせた適切な名前のフォルダーに配置します。

[start=4]
. 大量の情報を手で入力する代わりに、次のコマンドを実行して、サンプルの https://raw.githubusercontent.com/rhai-code/backToTheFuture/main/qna.yaml[`qna.yaml`] ファイルを分類ディレクトリにコピーするだけです。

[source,console,role=execute,subs=attributes+]
----
cp -av ~/files/backToTheFuture/qna.yaml /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims/
----

[start=5]
. 次に、ファイルの最初の 10 行を表示する次のコマンドを発行して、ファイルが正しくコピーされたことを確認できます。

[source,console,role=execute,subs=attributes+]
----
head /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims/qna.yaml
----

このワークショップでは、この情報をすべて手書きで入力することは期待されていません。参考のためにここに内容を記載します。

これは、トレーナーモデルが学生モデルを指導するために使用する Q&A サンプルのリストで構成される YAML ファイルです。 git 内のテキスト ファイルの特定のコミットへのリンクであるソース ドキュメントもあります。 https://github.com/gshipley/backToTheFuture/blob/main/data.md[ここ] に磁束コンデンサの手頃な価格が 10,000,000 ドルだということが含まれています。

qna ファイル形式を理解しやすくするために、以下にファイルの抜粋を示します。次のコマンドを使用して、システム上のファイル全体を表示してください。

[source,console,role=execute,subs=attributes+]
----
cat /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims/qna.yaml
----

[source,yaml]
----
version: 3
domain: time_travel
created_by: Marty McFly
seed_examples:
  - context: |
      The DeLorean DMC-12 is a sports car manufactured by John DeLorean's DeLorean Motor Company
      for the American market from 1981 to 1983. The car features gull-wing doors and a stainless-steel body.
      It gained fame for its appearance as the time machine in the "Back to the Future" film trilogy.
    questions_and_answers:
      - question: |
          When was the DeLorean manufactured?
        answer: |
          The DeLorean was manufactured from 1981 to 1983.
      - question: |
          Who manufactured the DeLorean DMC-12?
        answer: |
          The DeLorean Motor Company manufactured the DeLorean DMC-12.
      - question: |
          What type of doors does the DeLorean DMC-12 have?
        answer: |
          Gull-wing doors.
document_outline: |
  Details and repair costs on a DeLorean DMC-12 car.
document:
  repo: https://github.com/gshipley/backToTheFuture.git
  commit: 8bd9220c616afe24b9673d94ec1adce85320809c
  patterns:<6>
    - data.md
----

. `**version**`: qna.yaml ファイルのバージョン。これは、SDG (Synthetic Data Generation) に使用されるファイルの形式です。値は数値 3 である必要があります。
. `**created_by**`: 作者の GitHub ユーザー名
. `**domain**`: ナレッジのカテゴリ
. `**seed_examples**`: key/value エントリのコレクション
.. `**context**`: ナレッジ ドキュメントからの情報。各 qna.yaml には 5 つのコンテキスト ブロックが必要で、最大単語数は 500 単語です。
.. `**questions_and_answers**`: 質問と回答
... `**question**`: モデルに対する質問。各 qna.yaml ファイルには、コンテキスト チャンクごとに少なくとも 3 つの質問と回答のペアが必要で、最大単語数は 250 ワードです。
... `**answer**`: モデルに対して要求される答え。各 qna.yaml ファイルには、コンテキスト チャンクごとに少なくとも 3 つの質問と回答のペアが必要で、最大単語数は 250 ワードです。
. `**document_outline**`: 提出する書類の概要を説明します。
. `**document**`: 知識貢献の情報源
.. `**repo**`: ナレッジ マークダウン ファイルを保持するリポジトリの URL
.. `**commit**`: ナレッジ マークダウン ファイルを含むリポジトリ内のコミットの SHA。
.. `**patterns**`: リポジトリ内のマークダウン ファイルを指定する glob パターンのリスト。 *.md などの * で始まる glob パターンは、YAML ルールにより引用符で囲む必要があります。たとえば、*.md です。

次に、シード データが適切にキュレーションされていることを確認します。

[start=6]
. 分類法を検証する

InstructLab を使用すると、追加データを生成する前に分類ファイルを検証できます。これを行うには、以下に示すように `ilab Taxonomy diff` コマンドを使用します。:

NOTE: コマンド ラインの (venv) で示される仮想環境にまだいることを確認してください。そうでない場合は、venv/bin/activate ファイルを再度取得します。

[source,console,role=execute,subs=attributes+]
----
ilab taxonomy diff
----

.You should see the following output:
[subs:quotes]
----
knowledge/parasol/claims/qna.yaml
Taxonomy in /home/instruct/.local/share/instructlab/taxonomy is valid :)
----

[#sdg]
=== 合成データの生成

さて、ここまでは順調です。さて、素晴らしい部分に移りましょう。 `qna.yaml` ファイルを含む分類法を使用して、LLM にさらに多くの例を自動的に生成させます。生成ステップには時間がかかることが多く、生成する命令の数によって異なります。

InstructLab は、提供されたサンプルに基づいて X 個の追加の質問と回答を生成します。たとえば、デフォルトの完全合成データ生成パイプラインをスケール ファクター 30 で実行すると、7 分かかります。これは、Apple Silicon を使用すると約 15 分かかる場合があり、多くの要因によって異なります。時間を短縮するため、またはハードウェアの規模が小さい場合は、スケール係数をカスタマイズしたり、単純なパイプラインを実行したりすることもできますが、最適な出力が生成されないため、お勧めできません。

ただし、このワークショップの目的では、どのように機能するかを理解していただくために、少量の追加サンプルのみを生成します。

NOTE: 必要に応じて、Granite モデルを実行しているターミナルで kbd:[CTRL+C] と入力して、Granite モデルの提供を停止します。

 (**bottom** （2つ目）のターミナル) でコマンドを実行して、合成データを生成します。 merlinite モデルは **教師** モデルとして機能します。

[source,console,role=execute,subs=attributes+]
----
ilab data generate --model /home/instruct/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf --sdg-scale-factor 5 --pipeline simple --gpus 1
----

このコマンドを実行すると、魔法が始まります。

NOTE: SDG プロセスが開始される前に `AssertionError` がスローされるのがわかります。これはプロセスには影響しませんので、安心して続行してください。

InstructLab は現在、`qna.yaml` ファイルで提供されたシード データに基づいてデータを合成的に生成しています。

以下に示すように、データが生成されていることを示す出力が画面に表示されます。

[subs:quotes]
----
INFO 2024-10-21 02:01:23,450 instructlab.sdg.llmblock:51: LLM server supports batched inputs: False
INFO 2024-10-21 02:01:23,450 instructlab.sdg.pipeline:197: Running block: gen_knowledge
INFO 2024-10-21 02:01:23,450 instructlab.sdg.pipeline:198: Dataset({
    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3'],
    num_rows: 10
})
----

これは完了するまでに数分かかります。

プロセスが完了し、追加のデータを生成したら、 `ilab model train` コマンドを使用して、このデータセットをモデルに組み込むことができます。

生成されたデータを確認したい場合は、SDG プロセスによって、knowledge_train_msgs[TIMESTAMP].jsonl という名前の jsonl ファイルが `/home/instruct/.local/share/instructlab/datasets` ディレクトリに作成されます。

TIP: JSONL ファイルは、それぞれが独自の行にある複数の JSON オブジェクトで構成されます。

ぜひ探索してみてください。次のコマンドに正確なファイル名を入力する必要があります。

[source,console]
----
cat /home/instruct/.local/share/instructlab/datasets/knowledge_train_msgs[YOUR_TIMESTAMP].jsonl
----

NOTE: 一般に、スケール係数 5 を使用しても、モデルの知識やスキルに効果的に影響を与えるのに十分な合成データはありません。ただし、このワークショップでは時間の制約があるため、実際のコマンドを使用してこれがどのように機能するかを簡単に説明することが目的です。通常、モデルを効果的にトレーニングするには、デフォルト値であるスケール係数 30 を使用します。

新しいデータが生成されたら、次のステップは、更新された知識を使用してモデルをトレーニングすることです。これは、 `ilab model train` コマンドを使用して実行されます。

NOTE: 新しく生成されたデータを使用したトレーニングは、時間とリソースを大量に消費するタスクです。必要なエポック数、セーフテンサーのダウンロードのためのインターネット接続、その他の要因によっては、何時間もかかる場合があり、使用するハードウェアに大きく依存します。

[#changing_model]
== InstructLab を使用した LLM の強化

このラボでは時間の制約があるため、実際にはモデルをトレーニングしません。これには、本格的な合成データ生成プロセスと、何時間もかかる可能性のあるトレーニングの実行が必要になります。おそらくお忙しいと思いますので、お待たせすることなく最終結果をご案内いたします。

. このプロセスをすでに経たモデルをデモ システムに提供しています。まず、いずれかのターミナル ウィンドウでプロセスが実行されている場合は、 kbd:[CTRL+C] を入力して終了します。新しくトレーニングされたモデルを提供するために、*top* コマンド ウィンドウで次のコマンドを実行できるようになりました。

[source,console,role=execute,subs=attributes+]
----
ilab model serve --model-path /home/instruct/files/summit-connect-merlinite-lab-Q4.gguf
----

開始までに数秒かかる場合がありますが、次のような見覚えのあるものが表示されるはずです。

[subs:quotes]
----
INFO 2024-10-20 17:24:33,497 instructlab.model.serve:136: Using model '/home/instruct/summit-connect-merlinite-lab-Q4.gguf' with -1 gpu-layers and 4096 max context size.
INFO 2024-10-20 17:24:33,497 instructlab.model.serve:140: Serving model '/home/instruct/summit-connect-merlinite-lab-Q4.gguf' with llama-cpp
INFO 2024-10-20 17:24:34,492 instructlab.model.backends.llama_cpp:232: Replacing chat template:
 {% for message in messages %}
{% if message['role'] == 'user' %}
{{ '<|user|>
' + message['content'] }}
{% elif message['role'] == 'system' %}
{{ '<|system|>
' + message['content'] }}
{% elif message['role'] == 'assistant' %}
{{ '<|assistant|>
' + message['content'] + eos_token }}
{% endif %}
{% if loop.last and add_generation_prompt %}
{{ '<|assistant|>' }}
{% endif %}
{% endfor %}
INFO 2024-10-20 17:24:34,495 instructlab.model.backends.llama_cpp:189: Starting server process, press CTRL+C to shutdown server...
INFO 2024-10-20 17:24:34,495 instructlab.model.backends.llama_cpp:190: After application startup complete see http://127.0.0.1:8000/docs for API.
----

[#verify]
=== アプリケーションの検証

さて、ここからが真実の瞬間だ。知識を追加し、合成データを生成し、モデルを再トレーニングしました。パラソル保険アプリケーションで Marty McFly の請求を表示していた *ブラウザ ウィンドウを更新* します。

image::parasol-view.png[]

画面の右下隅にある青いチャットボット アイコンをクリックしてチャットボットを開きます。すでに開いている場合は、チャット ウィンドウの左下隅にある小さな kbd:[+] ボタンを押して新しいセッションを開始する必要があります。

image::parasol-chat.webp[width=350]

. 新しくトレーニングされたモデルを使用してチャットボットに同じ質問をして、応答が改善されたかどうかを確認してみましょう。

[source,console,role=execute,subs=attributes+]
----
How much does it cost to repair a flux capacitor?
----

次のような内容が表示されるはずです (大規模言語モデルの性質により、出力が異なる場合があることに注意してください)。

image::parasol-new-response.webp[width=350]

*CONGRATULATIONS!* Parasol 保険用のチャットボットをトレーニングしただけで、すべての保険請求担当者の生活が少し改善されます。

[#conclusion]
== 結論

*WOOHOO!* 若いパダワン、任務は完了しました。少し息を吸ってください。私たちはあなたを誇りに思います。あえて言えば、あなたは今 AI エンジニアです。おそらく次のステップは何なのか疑問に思われていると思いますので、いくつかの提案をさせてください。

スキルと知識の両方を追加してプレイを始めてください。これはモデルに何か「新しい」ものを与えるためです。知らないデータの塊を与えて、それをもとにトレーニングします。 InstructLab でトレーニングされたモデルは、あなたの会社でどのように役立ちますか?最初にどの友達に自慢しますか?

ご覧のとおり、InstructLab は非常に簡単で、ほとんどの時間は新しい分類コンテンツのキュレーションに費やされます。繰り返しになりますが、ここまで進んでいただいたことをとてもうれしく思います。ご質問があれば、私たちがお手伝いいたします。皆さんが何を思いつくか楽しみにしています。

アップストリーム コミュニティに参加する方法については、公式プロジェクト (https://github.com/instructlab[www.github.com/instructlab]) にアクセスし、コミュニティ リポジトリをチェックしてください。また、RHEL AI の詳細については、 https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/ai[こちら] をご覧ください(RHEL AIには、InstructLab のサポート、含まれる Granite 大規模言語モデルのモデル出力の識別化、ハイブリッド クラウド上で独自の方法で AI を実行するプラットフォームが含まれます）。

