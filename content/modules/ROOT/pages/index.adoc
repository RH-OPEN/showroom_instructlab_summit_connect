= RHEL AI and InstructLab: Get a taste in training your own model

:experimental: true
:imagesdir: ../assets/images
:toc: false
:numbered: true

== Introduction to the Lab

Red Hat Summit Connect で RHEL AI について学び、使用するために時間を割いていただきありがとうございます。この実践的な演習では、 https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/ai[RHEL AI] とは何か、そしてそれを使用して企業に価値を提供する方法を学びます。また、RHEL AI を活用して大規模言語モデル (LLM) を改善し、合成データ生成を使用してトレーニングする方法も学習します。  RHEL AI はいくつかのコアコンポーネントで構成されています。:

. https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/image-mode[RHEL Image Mode]
. The https://www.ibm.com/granite[Granite] Large Language Model(s)
. https://www.redhat.com/en/topics/ai/what-is-instructlab[InstructLab] for model alignment
. Support and indemnification from Red Hat

https://www.redhat.com/en/topics/ai/what-is-instructlab[InstructLab] は、Red Hat と MIT-IBM Watson AI Lab による完全なオープンソース プロジェクトです。MIT-IBM Watson AI Labは https://arxiv.org/abs/2403.01081[Large-scale Alignment for chatBots] (LAB)（チャットボットの大規模調整）を発表しました。このプロジェクトのイノベーションは、LLM トレーニングの指導調整段階で役立ちます。ただし、このプロジェクトの利点を完全に理解するには、LLM とは何か、モデルのトレーニングに関連する難しさとコストについて、いくつかの基本概念を理解しておく必要があります。

[#llms]
=== What is a Large Language Model?

大規模言語モデル (large language model: LLM) は、深層学習技術 (deep learning techniques) を使用して入力データに基づいて人間のようなテキストを理解して生成する人工知能 (artificial intelligence: AI) モデルの一種です。これらのモデルは、膨大な量のテキスト データを分析し、データ内のパターン、関係、構造を学習するように設計されています。これらは、次のようなさまざまな自然言語処理 (natural language processing: NLP) タスクに使用できます。:

* *Text classification*: スパム検出や感情分析など、内容に基づいてテキストを分類します。
* *Text summarization*: ニュース記事や研究論文など、長いテキストの簡潔な要約を生成します。
* *Machine translation*: 英語からフランス語、ドイツ語から中国語など、ある言語から別の言語へのテキストの翻訳します。
* *Question answering*: 特定のコンテキストまたは一連の文書に基づいて質問に回答します。
* *Text generation*: 記事、物語、さらには詩を書くなど、一貫性があり、文脈に関連性があり、文法的に正しい新しいテキストを作成します。

大規模な言語モデルには通常、データ内の複雑な言語パターンと関係を捉えることを可能にする多くのパラメーター (数百万から数十億) があります。これらは、教師なし事前トレーニングや教師あり微調整などの手法を使用して、書籍、記事、Web サイトなどの大規模なデータセットでトレーニングされます。人気のある大規模言語モデルには、GPT-4、Llama、Mistral などがあります。

要約すると、大規模言語モデル (LLM) は、深層学習技術を使用して、入力データに基づいて人間のようなテキストを理解して生成する人工知能モデルです。これらは、膨大な量のテキスト データを分析し、データ内のパターン、関係、構造を学習するように設計されており、さまざまな自然言語処理タスクに使用できます。

NOTE: LLM が何を達成できるかを理解していただくために、このセクション全体は、ワークショップで使用している基本モデルに対する簡単な質問で生成しました。

[#how_trained]
=== How are Large Language Models trained?

大規模言語モデル (LLM) は通常、深層学習技術と大規模なデータセットを使用してトレーニングされます。トレーニングプロセスにはいくつかのステップが含まれます。:

. *Data Collection*: 膨大な量のテキスト データが書籍、記事、Web サイト、データベースなどのさまざまなソースから収集されます。モデルを適切に一般化できるように、データにはさまざまな言語、ドメイン、スタイルが含まれる場合があります。
. *Pre-processing*: 生のテキスト データは前処理され、ノイズ、不一致、無関係な情報が除去されます。これには、トークン化、小文字化、ステミング、見出し語化、およびエンコードが含まれる場合があります。
. *Tokenization*: 前処理されたテキスト データは、モデルへの入力および出力として使用できるトークン (単語またはサブワード) に変換されます。一部のモデルでは、バイト ペア エンコーディング (BPE) またはサブワード セグメンテーションを使用して、語彙外の単語を処理し、コンテキスト情報を維持できるトークンを作成します。
. *Pre-training*: モデルは教師なしまたは自己教師ありの方法でトレーニングされ、データ内のパターンと構造を学習します。
. *Model Alignment*: (命令チューニングとプリファレンスチューニング): 人間の価値観と目標を大規模な言語モデルにエンコードして、可能な限り役立つ、安全、信頼できるものにするプロセス。このステップは、他の一部のステップほど計算集約的ではありません。

[#instructlab]
=== How does this relate to InstructLab?

InstructLab は、分類学に基づいた合成データ生成プロセスとマルチフェーズ チューニングフレームワークを活用しています。これにより、InstructLab は高価な人間による注釈への依存を大幅に削減し、大規模な言語モデルへの貢献が簡単かつアクセスしやすくなります。これは、InstructLab が LLM を使用してデータを生成し、そのデータを使用して LLM をさらにトレーニングできることを意味します。これは、調整フェーズがほとんどのユーザーにとって知識を提供するための出発点になることも意味します。  LAB 手法が導入される前は、通常、ユーザーは LLM のトレーニングに直接関与していませんでした。複雑に聞こえるかもしれませんが、ちょっと待ってください、これからこれがいかに使いやすいかがわかるでしょう。

InstructLab を使用していると、スキルと知識という用語が表示されます。スキルと知識の違いは何ですか?簡単な例えとしては、スキルを誰かに釣り方を教えることだと考えてください。一方、知識とは、バスを釣るのに、最適な場所は日が沈む時間帯で、岸沿いの木の幹の近くにラインをキャストすることを知っていることです。

[#getting_started]
== Getting started with InstructLab

=== Install the ilab command line interface

私たちは、ローカル LLM 開発者エクスペリエンスとワークフローを実装する https://github.com/instructlab/instructlab[ilab] という CLI ツールを作成しました。 ilab CLI は Python で書かれており、次のアーキテクチャで動作します。:

. Apple M1/M2/M3 Mac
. Linux systems

将来的には、さらに多くのオペレーティング システムがサポートされる予定です。コマンドラインツールを使用するためのシステム要件は次のとおりです。:

. C++ compiler
. Python 3.9+
. Approximately 60GB disk space (entire process)
.. ディスク容量の要件は、いくつかの要因によって異なります。モデルをシステム上にローカルに置きながら、モデルにフィードするデータを生成することに注意してください。たとえば、このワークショップで使用しているモデルのサイズは約 5 GB です。

[#installation]
=== Installing ilab

最初に行う必要があるのは、InstructLab コマンド ライン ツールと対話できるようにする Python 仮想環境を調達することです。右側に 2 つのターミナル ウィンドウが表示されますが、*上の* ウィンドウから始めましょう。

. 次のコマンドを実行して、Python 仮想環境をアクティブ化します。:
+

[source,sh,role=execute,subs=attributes+]
----
cd ~/instructlab
source venv/bin/activate
----
+
.プロンプトは次のようになります。

[subs=quotes]
----
(venv) [instruct@instructlab instructlab]$
----
+

// . Install the command line tool using the pip command
// +

// [source,sh,role=execute,subs=attributes+]
// ----
// pip3 install git+https://github.com/instructlab/instructlab.git@v0.17.1

// ----
// +

// NOTE: インターネット接続やファイルがローカルにキャッシュされているかどうかによっては、 `pip install` に時間がかかる場合があります。

[start=2]
. venv 環境から ilab コマンドを実行して、ilab が正しくインストールされていることを確認します。
+

[source,sh,role=execute,subs=attributes+]
----
ilab
----
+

すべてが正しくインストールされていると仮定すると、次の出力が表示されるはずです。:
+

[subs=quotes]
----
Usage: ilab [OPTIONS] COMMAND [ARGS]...


  CLI for interacting with InstructLab.


  If this is your first time running ilab, it's best to start with `ilab init`
  to create the environment.


Options:
  --config PATH  Path to a configuration file.  [default: config.yaml]
  --version      Show the version and exit.
  --help         Show this message and exit.


Commands:
  chat      Run a chat using the modified model
  check     (Deprecated) Check that taxonomy is valid
  convert   Converts model to GGUF
  diff      Lists taxonomy files that have changed since <taxonomy-base>...
  download  Download the model(s) to train
  generate  Generates synthetic data to enhance your example data
  init      Initializes environment for InstructLab
  list      (Deprecated) Lists taxonomy files that have changed since <taxonomy-base>.
  serve     Start a local server
  test      Runs basic test to ensure model correctness
  train     Takes synthetic data generated locally with `ilab generate`...
----


*CONGRATULATIONS!* これですべてがインストールされ、LLM アライメントの世界に飛び込む準備が整いました。

[#initialize]
=== Initialize ilab

コマンドライン インターフェイス「ilab」が正しく動作していることがわかったので、次に行う必要があるのは、モデルの操作を開始できるようにローカル環境を初期化することです。これは、単純な init コマンドを発行することで実現されます。次のコマンドを実行して「ilab」を初期化します。:

[source,sh,role=execute,subs=attributes+]
----
ilab config init
----

.次の出力が表示されるはずです:

[subs=quotes]
----
Welcome to InstructLab CLI. This guide will help you to setup your environment.
Please provide the following values to initiate the environment [press Enter for defaults]:
Path to taxonomy repo [taxonomy]:
----

IMPORTANT: オプションを設定して分類法をダウンロードするように求められますが、ここでは kbd:[ENTER] だけを入力してデフォルト設定で進みます。

// When prompted to accept the `config.yaml`, hit kbd:[ENTER]

// [subs=quotes]
// ----
// Path to taxonomy repo [taxonomy]:
// ----

// NOTE: When prompted to provide the path to the taxonomy repo, hit kbd:[ENTER] 

// [subs=quotes]
// ----
// `taxonomy` seems to not exist or is empty. Should I clone git@github.com:instruct-lab/taxonomy.git for you? [y/N]: y
// ----

// NOTE: If asked if the CLI should clone the taxonomy repo, input 'y' as shown in the above output.

// [source,sh]
// ----
// Path to your model [models/merlinite-7b-lab-Q4_K_M.gguf]:
// ----

// NOTE: モデル ファイルのディレクトリを入力するように求められたら、デフォルトを使用して、 <ENTER> を入力してください。

[subs=quotes]
----
Generating `config.yaml` in the current directory...
Initialization completed successfully, you're ready to start using `ilab`. Enjoy!
----

* 初期化フェーズではいくつかの処理が行われます。デフォルトの分類法がローカル ファイル システムに配置され、構成ファイル (config.yaml) が現在のディレクトリに作成されます。
* config.yaml ファイルには、このワークショップ中に使用するデフォルトが含まれています。このワークショップの後、InstructLab を試し始めるときは、パラメーターを好みに合わせて調整できるように、構成ファイルの内容を理解することが重要です。

[#download]
=== Download the model

InstructLab 環境を構成すると、量子化 (圧縮および最適化された) モデルをローカル ディレクトリにダウンロードして、API リクエストのモデル サーバーとして使用したり、このワークショップで行うように新しいモデルのトレーニングに使用したりできるようになります。 `ilab model download` を実行します。

[source,sh,role=execute,subs=attributes+]
----
ilab model download --repository instructlab/granite-7b-lab-GGUF --filename=granite-7b-lab-Q4_K_M.gguf
----

`ilab model download` コマンドは、このワークショップで使用するモデルを HuggingFaceのinstructlab 組織からダウンロードします。出力は次のようになります。:

// NOTE: *モデルがローカル マシンにキャッシュされている場合、このコマンドでは内容が表示されない場合があります。*

[subs=quotes]
----
Downloading model from downloading model from instructlab/granite-7b-lab-GGUF@main to models...
Downloading 'granite-7b-lab-Q4_K_M.gguf' to 'models/.huggingface/download/granite-7b-lab-Q4_K_M.gguf.6adeaad8c048b35ea54562c55e454cc32c63118a32c7b8152cf706b290611487.incomplete'
INFO 2024-05-06 16:46:24,394 file_download.py:1877 Downloading 'granite-7b-lab-Q4_K_M.gguf' to 'models/.huggingface/download/granite-7b-lab-Q4_K_M.gguf.6adeaad8c048b35ea54562c55e454cc32c63118a32c7b8152cf706b290611487.incomplete'100%|█████████████████████████████████████████████████████████████| 4.08G/4.08G [00:36<00:00, 110MB/s]
----

モデルがダウンロードされたので、モデルを提供してチャットできるようになります。モデルを提供するということは、他のプログラムが API 呼び出しを行うのと同様にデータを操作できるようにするサーバーを実行することを意味します。

[#serve]
=== Serving the model

次のコマンドを実行してモデルを提供しましょう:

[source,sh,role=execute,subs=attributes+]
----
ilab model serve --model-path models/granite-7b-lab-Q4_K_M.gguf
----

ご覧のとおり、serve コマンドはオプションの `-–model-path` 引数を取ることができます。この場合、Granite モデルを提供したいと考えています。モデル パスが指定されていない場合は、config.yaml ファイルのデフォルト値が使用されます。 モデルが提供されて準備が完了すると、次の出力が表示されます。:

[subs=quotes]
----
INFO 2024-04-23 17:16:53,903 lab.py:296 Using model '/models/granite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size.
INFO 2024-04-23 17:17:02,861 server.py:155 Starting server process, press CTRL+C to shutdown server...
INFO 2024-04-23 17:17:02,861 server.py:156 After application startup complete see http://127.0.0.1:8000/docs for API.
----

*WOOHOO!* 初めてモデルを提供したばかりで、LLM と対話してこれまでの作業をテストする準備ができています。これから、モデルとチャットすることで実現します。

[#chat]
=== Chat with the model

1 つのターミナル ウィンドウでモデルを提供しているため、ilab チャット コマンドを実行するには、別のターミナル ウィンドウを使用し、Python 仮想環境を再アクティブ化する必要があります。

// Note: command + T キーボードの組み合わせを押すと、ターミナルで新しいタブを開くことができます。サポートが必要な場合は、InstructLab ラウンジの Red Hatter にお問い合わせください。

. *下の* ターミナル ウィンドウで、次のコマンドを発行します。:

[source,sh,role=execute,subs=attributes+]
----
cd ~/instructlab
source venv/bin/activate
----
.Your Prompt should now look like this
[source,sh]
----
(venv) [instruct@instructlab instructlab]$ 
----

[start=2]
. 環境が提供されたので、`ilab model chat` コマンドを使用してチャット セッションを開始できます。:

[source,sh,role=execute,subs=attributes+]
----
ilab model chat -m models/granite-7b-lab-Q4_K_M.gguf
----

以下の例のようなチャット プロンプトが表示されるはずです。

[subs=quotes]
----
╭────────────────────────────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ MODELS/GRANITE-7B-LAB-Q4_K_M.GGUF (type /h for help)│                                                                                                                           
╰────────────────────────────────────────────────────────────────────────────────────╯
>>> 
----

[start=3]
. この時点で、モデルに質問することでモデルと対話できるようになります。
例：What is openshift in 20 words or less?（openshift を 20 語以内で説明してください。）
（現状InstructLabは日本語の入力をサポートしていないので英語での入力をお勧めします。）

[source,sh,role=execute,subs=attributes+]
----
What is openshift in 20 words or less?
----

待って、あれ？最高!!!!!これで、このマシン上で独自のローカル LLM が実行されました。とても簡単でしたね。

[#integrating_instructlab]
== Integrating AI into an Insurance Application

前のセクションでは、InstructLab と対話する方法の基本を説明しました。次に、サンプル アプリケーションで InstructLab を使用して、さらに一歩進んでみましょう。 RHEL AI を使用して granite LLM を活用し、知識やスキルの形でデータを追加し、新しい知識でモデルをトレーニングし、質問に効果的に回答できるようにします。これは、保険請求（insurance claims）を処理する架空の会社である Parasol のコンテキストで行われます。

Parasol には、AI (graniteモデル) を組み込んだチャットボット アプリケーションがあり、提出された請求に対して修理提案を提供します。これにより、Parasol は保留中のさまざまな請求の処理を迅速化できるようになります。しかし現時点では、チャットボットは効果的な修理提案を提供しません。さまざまな条件下で実行されたさまざまな修理を含む過去の請求データを使用して、ユーザーがこの知識をgraniteモデルに追加し、追加の知識に基づいてトレーニングし、推奨事項を改善する方法を示します。

[#using_parasol_application]
=== Using the Parasol Application

まず、請求担当者がチャットボットと対話する際の現在のエクスペリエンスを見てみましょう。

.  *Terminals* ビューにいる場合、 *Parasol* に切り替えて、ブラウザーに Parasol 会社のクレーム アプリケーションを表示します。

image::parasol-view.png[]

請求担当者は、画面上の請求番号をクリックすると、既存の請求に移動して表示できます。

[start=2]
. このラボでは、Marty McFly によって提出された請求である *CLM195501* を調査します。この請求をクリックしてみましょう。

image::parasol-claim.png[]

このページで請求の詳細を読むことができ、デロリアンの画像を拡大して、マーティが乗り物をどれほどひどく破壊したかを確認することもできます（地面にある磁束コンデンサに注目してください）。

[start=3]
. 請求内容を読んだら、ページの右下にある小さな青いアイコンを使用してチャットボットをクリックします。

image::parasol-chat.webp[width=350]

IMPORTANT: このチャットボットは、以前に提供した Granite モデルによってサポートされているため、実行中のプロセスを強制終了した場合は、次のコマンドを実行してターミナルで再起動する必要があります: `ilab model serve --model-path models/granite-7b-lab-Q4_K_M.gguf`

// [source,sh,role=execute,subs=attributes+]
// ----
// ilab model chat -m models/granite-7b-lab-Q4_K_M.gguf
// ----

あなたが保険請求担当者として、マーティのデロリアンの磁束コンデンサの修理にどれくらいの費用がかかるかを知りたいと考えていると想像してください。 

[start=4]
. チャットボットに次の質問をしてください:

[source,sh,role=execute,subs=attributes+]
----
How much does it cost to repair a flux capacitor?
----

次のような内容が表示されるはずです。 LLM は本質的に非決定的であることに注意してください。これは、同じプロンプト入力であっても、モデルがさまざまな応答を生成することを意味します。したがって、結果は若干異なる場合があります。

image::parasol-chat-response.webp[width=350]

私たちがすでに始めていることは、プロンプト エンジニアリングを使用して、LLM との各会話で請求に関するコンテキスト情報を提供することです。しかし、残念ながら、チャットボットは磁束コンデンサの修理にどれくらいの費用がかかるのか、または私たちの組織のドメイン固有の知識を知りません。 InstructLab と RHEL AI を使用すると、モデルを教えることでこの状況を変えることができます。

[#taxononmy]
=== Understanding the Taxonomy

InstructLab は、大規模言語モデル (LLM) に新しい合成データによるアライメント調整手法を使用します。InstructLab の「lab」は、**L**arge-scale **A**lignment for Chat **B**ots を表します。 LAB メソッドは分類法によって駆動され、分類法は主に手作業で慎重に作成されます。

InstructLab は、新しい InstructLab オープンソース コミュニティで知識とスキルという 2 種類のデータを収集することで、モデルのチューニングと改善のプロセスをクラウドソーシングします。それらの提供物は、分類されたYAML ファイルとして収集され、合成データ生成プロセスで使用されます。分類のディレクトリ構造を理解するには、次の画像を参照してください。

image::taxonomy.png[]

これから、分類モデルを活用して、組織の公開 (および非公開) 内部データのコレクションから、対象となる特定の車両とその詳細に関する知識をモデルに教えます。

*Terminals* ビューに戻り、チャットを実行しているターミナル ウィンドウで、「exit」と入力してチャット セッションを終了します。

. Navigate to the taxonomy directory.

[source,sh,role=execute,subs=attributes+]
----
cd ~/instructlab
tree taxonomy | head -n 10
----

.以下に示すように分類ディレクトリがリストされているはずです。:
[source,texinfo]
----
taxonomy/
├── CODE_OF_CONDUCT.md
├── compositional_skills
│   ├── arts
│   ├── engineering
│   ├── geography
│   ├── grounded
│   │   ├── arts
│   │   ├── engineering
│   │   ├── geography
----

次に、ファイルを配置できるディレクトリを作成する必要があります。

[start=2]
. Instructlab でナレッジを適切に生成する方法を示す新しいナレッジを追加するディレクトリを作成します。

[source,sh,role=execute,subs=attributes+]
----
mkdir -p ~/instructlab/taxonomy/knowledge/parasol/claims
----

[start=3]
. 新しい知識を通じてモデルに新しい機能を追加します。 

分類アプローチが機能する方法は、質問と回答のサンプル データ セットを含む `qna.yaml` という名前のファイルを提供することです。このデータ セットは、モデルの出力に完全に影響を与えるのに十分な、さらに多くの合成データ サンプルを作成するプロセスで使用されます。  `qna.yaml` ファイルについて理解する重要な点は、InstructLab がそれを使用してより多くの例を合成的に生成するには、このファイルが特定のスキーマに従う必要があるということです。

`qna.yaml` ファイルは、分類ディレクトリの `knowledge` サブディレクトリ内のフォルダーに配置されます。以下のコマンドでわかるように、データ トピックに合わせた適切な名前のフォルダーに配置されます。

[start=4]
. 大量の情報を手で入力する代わりに、次のコマンドを実行して、サンプルの https://raw.githubusercontent.com/rhai-code/backToTheFuture/main/qna.yaml[`qna.yaml`] ファイルを分類ディレクトリにコピーするだけです。:

[source,sh,role=execute,subs=attributes+]
----
cp -av ~/files/qna.yaml ~/instructlab/taxonomy/knowledge/parasol/claims/
----

[start=5]
. 次に、ファイルの最初の 10 行を表示する次のコマンドを発行して、ファイルが正しくコピーされたことを確認できます。:

[source,sh,role=execute,subs=attributes+]
----
head ~/instructlab/taxonomy/knowledge/parasol/claims/qna.yaml
----

このワークショップでは、この情報をすべて手書きで入力することは期待されていません。参考のためにここに内容を記載します。

これは、トレーナーモデルが学生モデルを指導するために使用する Q&A サンプルのリストで構成される YAML ファイルです。 git 内のテキスト ファイルの特定のコミットへのリンクであるソース ドキュメントもあります。 https://github.com/gshipley/backToTheFuture/blob/main/data.md[ここ] に磁束コンデンサの手頃な価格が 10,000,000 ドルだということが含まれています。 Q&A ファイル形式を理解しやすくするために、以下に Q&A ファイル形式を示します。

[source,yaml]
----
created_by: Marty_McFly<1>
domain: parasol<2>
seed_examples:<3>
- answer: The DeLorean was manufactured from 1981 to 1983.
  question: When was the DeLorean manufactured?
- answer: The DeLorean Motor Company manufactured the DeLorean DMC-12.
  question: Who manufactured the DeLorean DMC-12?
- answer: Transmission Repair costs between $2,500 and $4,000 for the Delorean DMC-12.
  question: How much does it cost to repair the transmission on a DeLorean DMC-12?
- answer: The top speed of the DeLorean DMC-12 was 110MPH and the 0-60 time was approximately 8.8 seconds.
  question: How fast was the Delorean DMC-12?
- answer: The DeLorean DMC-12 weighs 2,712lb (1,230kg).
  question: How much does the DeLorean DMC-12 weigh?
- answer: Maintenance on a DeLorean DMC-12 includes regular oil changes every 3,000 miles or 3 months,
    brake fluid change every 2 years, transmission fluid changes every 30,000 miles, coolant change every 2 years,
    and regularly checking the battery for corrosion and proper connection.
  question: What does maintenance for a DeLorean DMC12 look like?
- answer: It costs between $800 and $1000 to repair the suspension on a DeLorean DMC-12.
  question: How much does it cost to repair the supension on a DeLorean DMC-12?
task_description: 'Details on instructlab community project'<4>
document:<5>
  repo: https://github.com/gshipley/backToTheFuture.git
  commit: 8bd9220c616afe24b9673d94ec1adce85320809c
  patterns:<6>
    - data.md
----

<1> `created_by`: 投稿の作成者 (通常は GitHub ユーザー名)
<2> `domain`: 知識のカテゴリー
<3> `seed_examples`: 提供されたナレッジ ドキュメントから得られる 5 つ以上の、モデルに対する `question` と望ましい `response` の例
<4> `task_description`: 特定の知識を容易に理解するための知識のオプショナルの説明
<5> `document`: 知識のソース。ナレッジ マークダウン ファイルを指す `repo` URL と、ファイルを含む `commit` SHA で構成される。
<6> `patterns`: リポジトリ内のマークダウン ファイルを指定するパターンのリスト。

[start=6]
. 次に、シード データが適切にキュレーションされていることを確認します。

InstructLab を使用すると、追加データを生成する前に分類ファイルを検証できます。これを行うには、以下に示すように `ilab Taxonomy diff` コマンドを使用します。:

NOTE: コマンド ラインの (venv) で示される仮想環境にまだいることを確認してください。そうでない場合は、venv/bin/activate ファイルを再度取得します。

[source,sh,role=execute,subs=attributes+]
----
ilab taxonomy diff
----

.次の出力が表示されるはずです:
[source,sh]
----
knowledge/parasol/claims/qna.yaml
Taxonomy in /taxonomy/ is valid :)
----

[#synthetic_data]
=== Generating Synthetic Data

さて、ここまでは順調です。さて、素晴らしい部分に移りましょう。 `qna.yaml` ファイルを含む分類法を使用して、LLM にさらに多くの例を自動的に生成させます。生成ステップには時間がかかることが多く、生成する命令の数によって異なります。言い換えれば、これは、InstructLab が提供されたサンプルに基づいて X 個の追加の質問と回答を生成することを意味します。これにどれくらいの時間がかかるかを説明すると、適切な仕様のコンシューマ グレードの GPU アクセラレーション Linux マシンを使用した場合、100 個の追加の質問と回答を生成するには通常約 7 分かかります。 Apple Silicon を使用すると、これには約 15 分かかる場合があり、多くの要因によって異なります。このワークショップでは、追加のサンプルを 5 つだけ生成します。これを行うには、次のコマンドを発行します。:

. コマンドを実行して、現在のターミナルでデータを生成します。 *上の* ターミナルは以前の Granite モデルを引き続き提供しているはずです。モデルが提供されなくなった場合は、次のコマンド (他の *上の* ウィンドウで) を使用してモデルを再度提供します。

[source,sh,role=execute,subs=attributes+]
----
ilab model serve --model-path models/granite-7b-lab-Q4_K_M.gguf
----

[start=2]
. 次に、モデルを提供していないターミナル ウィンドウ、または *下の* ウィンドウで次のコマンドを実行します。:

[source,sh,role=execute,subs=attributes+]
----
cd ~/instructlab
ilab data generate --model models/granite-7b-lab-Q4_K_M.gguf --num-instructions 5
----

このコマンドを実行すると、InstructLab が qna.yaml ファイルで指定したシード データに基づいて 5 つのサンプルを合成的に生成していることがわかります。生成された質問と回答を見て、モデルが何を作成したかを確認してください。

[source,sh]
----
Generating synthetic data using 'models/granite-7b-lab-Q4_K_M.gguf' model, taxonomy:'taxonomy' against http://127.0.0.1:8000/v1 server
Cannot find prompt.txt. Using default prompt depending on model-family.
  0%|                                                                                                                                       | 0/5 [00:00<?, ?it/s]Synthesizing new instructions. If you aren't satisfied with the generated instructions, interrupt training (Ctrl-C) and try adjusting your YAML files. Adding more examples may help.
INFO 2024-08-06 16:38:53,754 generate_data.py:505: generate_data Selected taxonomy path knowledge->parasol->claims
Q> What is the horsepower of the DeLorean DMC-12?
I> 
A> The DeLorean DMC-12 has a horsepower of 130 hp.
----

追加データを生成したので、 `ilab model train` コマンドを使用して、このデータ セットをモデルに組み込むことができます。

NOTE: 通常、追加の 5 つの例を生成しても、モデルの知識やスキルに効果的に影響を与えるには十分ではありません。ただし、このワークショップでは時間の制約があるため、実際のコマンドを使用してこれがどのように機能するかを簡単に説明することが目的です。通常は、100 個、さらには 1000 個の追加データ ポイントを生成する必要があります。それでも、ラップトップでのトレーニングは、実稼働 LLM をトレーニングするために行うものというよりは、テクノロジーのデモンストレーションにすぎません。実稼働 LLM のトレーニングのために、Red Hat は RHEL AI と OpenShift AI を提供します。

新しいデータが生成されたら、次のステップは、更新された知識を使用してモデルをトレーニングすることです。これは、 `ilab model train` コマンドを使用して実行されます。

NOTE: 新しく生成されたデータを使用したトレーニングは、時間とリソースを大量に消費するタスクです。必要な反復回数、セーフテンソルのダウンロードのためのインターネット接続、その他の要因に応じて、20 分から最大 1 時間かかる場合があります。 2500 命令と 300 回の反復を含む生成ステップを使用して作成された、すでにトレーニング済みのモデルを使用するため、ラボを続行するためにモデルをトレーニングする必要はありません。

[#training]
=== Training and Interacting with the Model

このラボでは時間の制約があるため、実際にはモデルをトレーニングしません。トレーニング済みのモデルが提供されます。ただし、トレーニングがどのように機能するかを説明するには、次のコマンドを発行します。:

[source,sh,subs=attributes+]
----
ilab model train --iters 10 --device cuda
----

実際にモデルをトレーニングしている場合は、次の出力が表示されます。: 

[source,sh]
----
LINUX_TRAIN.PY: NUM EPOCHS IS:  1
LINUX_TRAIN.PY: TRAIN FILE IS:  taxonomy_data/train_gen.jsonl
LINUX_TRAIN.PY: TEST FILE IS:  taxonomy_data/test_gen.jsonl
LINUX_TRAIN.PY: Using device 'cuda:0'
  NVidia CUDA version: 12.1
  AMD ROCm HIP version: n/a
  cuda:0 is 'NVIDIA A10G' (15.3 GiB of 22.1 GiB free, capability: 8.6)
  WARNING: You have less than 18253611008 GiB of free GPU memory on '{index}'. Training may fail, use slow shared host memory, or move some layers to CPU.
  Training does not use the local InstructLab serve. Consider stopping the server to free up about 5 GiB of GPU memory.
LINUX_TRAIN.PY: LOADING DATASETS
Generating train split: 5 examples [00:00, 265.43 examples/s]
Generating train split: 7 examples [00:00, 6582.99 examples/s]
/home/instruct/instructlab/venv/lib64/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LINUX_TRAIN.PY: NOT USING 4-bit quantization
LINUX_TRAIN.PY: LOADING THE BASE MODEL
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  5.95it/s]
----

. このプロセスが完了するまでに 1 時間以上かかるため、このプロセスがすでに完了したモデルを提供しています。まず、*上の* ウィンドウで kbd:[CTRL+C] を使用して、現在のモデルサーバーを停止します。新しくトレーニングされたモデルを提供するために、*上の* コマンド ウィンドウで次のコマンドを実行します。。

[source,sh,role=execute,subs=attributes+]
----
ilab model serve --model-family merlinite --model-path /home/instruct/summit-connect-merlinite-lab-Q4.gguf
----

開始までに数秒かかる場合がありますが、次のように表示されます。:

[source,sh]
----
INFO 2024-08-06 17:04:12,748 serve.py:51: serve Using model 'models/summit-connect-merlinite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size.
INFO 2024-08-06 17:04:15,452 server.py:218: server Starting server process, press CTRL+C to shutdown server...
INFO 2024-08-06 17:04:15,452 server.py:219: server After application startup complete see http://127.0.0.1:8000/docs for API.
----

[#verify]
=== Verifying the Application

さて、ここからが真実の瞬間だ。知識を追加し、合成データを生成し、モデルを再トレーニングしました。パラソル保険アプリケーションで Marty McFly の請求を表示していたブラウザ ウィンドウを更新します。

image::parasol-view.png[]

画面の右下隅にある青いチャットボット アイコンをクリックしてチャットボットを開きます。

image::parasol-chat.webp[width=350]

. 新しくトレーニングされたモデルを使用してチャットボットに同じ質問をして、応答が改善されたかどうかを確認してみましょう。

[source,sh,role=execute,subs=attributes+]
----
How much does it cost to repair a flux capacitor?
----

次のような内容が表示されるはずです (大規模言語モデルの性質により、出力が異なる場合があることに注意してください)。:

image::parasol-new-response.webp[width=350]

*CONGRATULATIONS!* Parasol 保険用のチャットボットをトレーニングしただけで、すべての保険請求担当者の生活が少し改善されます。

[#conclusion]
== Conclusion

*WOOHOO!* 若いパダワン、任務は完了しました。少し息を吸ってください。私たちはあなたを誇りに思います。あえて言えば、あなたは今 AI エンジニアです。おそらく次のステップは何なのか疑問に思われていると思いますので、いくつかの提案をさせてください。

スキルと知識の両方を追加してプレイを始めてください。これはモデルに何か「新しい」ものを与えるためです。知らないデータの塊を与えて、それをもとにトレーニングします。 InstructLab でトレーニングされたモデルは、あなたの会社でどのように役立ちますか?最初にどの友達に自慢しますか?

ご覧のとおり、InstructLab は非常に簡単で、ほとんどの時間は新しい分類コンテンツのキュレーションに費やされます。繰り返しになりますが、ここまで進んでいただいたことをとてもうれしく思います。ご質問があれば、私たちがお手伝いいたします。皆さんが何を思いつくか楽しみにしています。

アップストリーム コミュニティに参加する方法については、公式プロジェクト (https://github.com/instructlab[www.github.com/instructlab]) にアクセスし、コミュニティ リポジトリをチェックしてください。また、RHEL AI の詳細については、 https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/ai[こちら] をご覧ください(RHEL AIには、InstructLab のサポート、Granite モデル ファミリの識別、ハイブリッド クラウド上で AI をお好みの方法で実行するためのプラットフォームが含まれます）。
