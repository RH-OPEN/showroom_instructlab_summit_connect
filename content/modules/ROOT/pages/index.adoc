= RHEL AI and InstructLab: Get a taste in training your own model

:experimental: true
:imagesdir: ../assets/images
:toc: false
:numbered: true

== Introduction to the Lab

Thanks for taking the time to learn about and use RHEL AI at Red Hat Summit Connect. During this hands-on exercise, you will learn what https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/ai[RHEL AI] is and how you can use it to provide value to your company. You will also learn how to leverage RHEL AI to improve a Large Language Model (LLM), and train it using synthetic data generation.  RHEL AI consists of several core components:

. https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/image-mode[RHEL Image Mode]
. The https://www.ibm.com/granite[Granite] Large Language Model(s)
. https://www.redhat.com/en/topics/ai/what-is-instructlab[InstructLab] for model alignment
. Support and indemnification from Red Hat

https://www.redhat.com/en/topics/ai/what-is-instructlab[InstructLab] is a fully open-source project from Red Hat, and the MIT-IBM Watson AI Lab, that introduces https://arxiv.org/abs/2403.01081[Large-scale Alignment for chatBots] (LAB). The project's innovation helps during the instruction-tuning phase of LLM training. However, to fully understand the benefit of this project, you need to be familiar with some basic concepts of what an LLM is and the difficulty and cost associated with training a model.

[#llms]
=== What is a Large Language Model?

A large language model (LLM) is a type of artificial intelligence (AI) model that uses deep learning techniques to understand and generate human-like text based on input data. These models are designed to analyze vast amounts of text data and learn patterns, relationships, and structures within the data. They can be used for various natural language processing (NLP) tasks, such as:

* *Text classification*: Categorizing text based on its content, such as spam detection or sentiment analysis.
* *Text summarization*: Generating concise summaries of longer texts, such as news articles or research papers.
* *Machine translation*: Translating text from one language to another, such as English to French or German to Chinese.
* *Question answering*: Answering questions based on a given context or set of documents.
* *Text generation*: Creating new text that is coherent, contextually relevant, and grammatically correct, such as writing articles, stories, or even poetry.

Large language models typically have many parameters (millions to billions) that allow them to capture complex linguistic patterns and relationships in the data. They are trained on large datasets, such as books, articles, and websites, using techniques like unsupervised pre-training and supervised fine-tuning. Some popular large language models include GPT-4, Llama, and Mistral.

In summary, a large language model (LLM) is an artificial intelligence model that uses deep learning techniques to understand and generate human-like text based on input data. They are designed to analyze vast amounts of text data and learn patterns, relationships, and structures within the data, and can be used for various natural language processing tasks.

NOTE: To give you an idea of what an LLM can accomplish, the entire previous section was generated with a simple question against the foundational model you are using in this workshop.

[#how_trained]
=== How are Large Language Models trained?

Large language models (LLMs) are typically trained using deep learning techniques and large datasets. The training process involves several steps:

. *Data Collection*: A vast amount of text data is collected from various sources, such as books, articles, websites, and databases. The data may include different languages, domains, and styles to ensure the model can generalize well.
. *Pre-processing*: The raw text data is pre-processed to remove noise, inconsistencies, and irrelevant information. This may include tokenization, lowercasing, stemming, lemmatization, and encoding.
. *Tokenization*: The pre-processed text data is converted into tokens (words or subwords) that can be used as input and output to the model. Some models use byte-pair encoding (BPE) or subword segmentation to create tokens that can handle out-of-vocabulary words and maintain contextual information.
. *Pre-training*: The model is trained in an unsupervised or self-supervised manner to learn patterns and structures in the data.
. *Model Alignment*: (instruction tuning and preference tuning): The process of encoding human values and goals into large language models to make them as helpful, safe, and as reliable as possible. This step is not as compute intensive as some of the other steps.

[#instructlab]
=== How does this relate to InstructLab?

InstructLab leverages a taxonomy-guided synthetic data generation process and a multi-phase tuning framework. This allows InstructLab to significantly reduce reliance on expensive human annotations, making contributing to a large language model easy and accessible. This means that InstructLab can use the LLM to generate data, which is then used to further train the LLM. It also means that the alignment phase becomes most users' starting point for contributing their knowledge.  Prior to the LAB technique, users typically had no direct involvement in training an LLM. I know this may sound complicated, but hang in there. You will see how easy this is to use.

As you work with InstructLab, you will see the terms Skills and Knowledge. What is the difference between Skills and Knowledge? A simple analogy is to think of a skill as teaching someone how to fish. Knowledge, on the other hand, is knowing that the best place to catch a Bass is when the sun is setting while casting your line near the trunk of a tree along the bank.

[#getting_started]
== Getting started with InstructLab

=== Install the ilab command line interface

We created a CLI tool called https://github.com/instructlab/instructlab[ilab] that implements a local LLM developer experience and workflow. The ilab CLI is written in Python and works on the following architectures:

. Apple M1/M2/M3 Mac
. Linux systems
. Windows 11 within a WSL environment

During this lab, we will be using the ilab command line tools on a Red Hat Enterprise Linux system. We anticipate support for more operating systems in the future. The system requirements to use the command line tool are as follows:

. C++ compiler
. Python 3.10+
. Approximately 60GB disk space (entire process)
.. Disk space requirements are dependent on several factors. Keep in mind that we will be generating data to feed to the model while also having the model locally on our system. For example, the model we are working with during this workshop is roughly 5gb in size.

[#installation]
=== Configuring ilab

The first thing we need to do is to source a Python virtual environment that will allow us to interact with the InstructLab command line tools. While you have two terminal windows available on your right, let's start with the *upper* window. 

. Navigate to the preset InstructLab folder and activate the python virtual environment by running the following commands:
+

[source,sh,role=execute,subs=attributes+]
----
cd ~/instructlab
source venv/bin/activate
----
+
.Your prompt should now look like this

[source,sh,role=execute,subs=attributes+]
----
(venv) [instruct@instructlab instructlab]$
----
+

// . Install the command line tool using the pip command
// +

// [source,sh,role=execute,subs=attributes+]
// ----
// pip3 install git+https://github.com/instructlab/instructlab.git@v0.18.4

// ----
// +

// NOTE: `pip install` may take some time, depending on the internet connection available at the conference or if the files have been cached locally.

[start=2]
. InstructLab is already pre-installed. From your venv environment, verify ilab is installed correctly by running the ilab command.
+

[source,sh,role=execute,subs=attributes+]
----
ilab
----
+

Assuming that everything has been installed correctly, you should see the following output:
+

[subs:quotes]
----
Usage: ilab [OPTIONS] COMMAND [ARGS]...


  CLI for interacting with InstructLab.


  If this is your first time running ilab, it's best to start with `ilab config init`
  to create the environment.


Options:
  --config PATH  Path to a configuration file.  [default: /home/instruct/.config/instructlab/config.yaml]
  -v, --verbose  Enable debug logging (repeat for even more verbosity)
  --config PATH  Path to a configuration file.  [default: /home/instruct/.config/instructlab/config.yaml]
  -v, --verbose  Enable debug logging (repeat for even more verbosity)
  --version      Show the version and exit.
  --help         Show this message and exit.

Commands:
  config    Command Group for Interacting...
  data      Command Group for Interacting...
  model     Command Group for Interacting...
  system    Command group for all...
  taxonomy  Command Group for Interacting...

Aliases:
  chat      model chat
  convert   model convert
  diff      taxonomy diff
  download  model download
  evaluate  model evaluate
  generate  data generate
  init      config init
  list      model list
  serve     model serve
  sysinfo   system info
  test      model test
  train     model train
----


*Congratulations!* You now have everything installed and are ready to dive into the world of LLM alignment!

[#initialize]
=== Initialize ilab

Now that we know that the command-line interface `ilab` is working correctly, the next thing we need to do is initialize the local environment so that we can begin working with the model. This is accomplished by issuing a simple init command. Initialize `ilab` by running the following command:

[source,sh,role=execute,subs=attributes+]
----
ilab config init
----

You should see the following output (press kbd:[ENTER] for defaults):

[subs=quotes]
----
Welcome to InstructLab CLI. This guide will help you to setup your environment.
Please provide the following values to initiate the environment [press Enter for defaults]:
Path to taxonomy repo [taxonomy]:
----

NOTE: While traditionally you'll be prompted to configure options and download the taxonomy, here you can just it kbd:[ENTER] for the default settings.

The last step of the ilab configuration is to select a train profile. Train profiles are GPU specific profiles that enable accelerated training behavior. If you are on MacOS or a Linux machine without a dedicated GPU, please choose No Profile (CPU-Only) by hitting Enter. There are various flags you can utilize with individual ilab commands that will allow you to utilize your GPU if applicable. **For this lab**, we will a single NVIDIA L4 GPU. Select the L4_x4.yaml option at the following prompt.

----
Generating `/home/instruct/.config/instructlab/config.yaml` and `/home/instruct/.local/share/instructlab/internal/train_configuration/profiles`...
Please choose a train profile to use:
[0] No profile (CPU-only)
[1] A100_H100_x2.yaml
[2] A100_H100_x4.yaml
[3] A100_H100_x8.yaml
[4] L40_x4.yaml
[5] L40_x8.yaml
[6] L4_x8.yaml
[7] L4_x4.yaml
Enter the number of your choice [hit enter for the default CPU-only profile] [0]: 6
----

NOTE: When prompted to enter a training profile, enter 6 as shown in the above output.

[subs=quotes]
----
You selected: L4_x4.yaml
Initialization completed successfully, you're ready to start using `ilab`. Enjoy!
----

* Several things happen during the initialization phase: A default taxonomy is located on the local file system, and a configuration file (config.yaml) is created in the 'home/instruct/.config/instructlab/' directory.
* The config.yaml file contains defaults we will use during this workshop. After this workshop, when you begin playing around with InstructLab, it is important to understand the contents of the configuration file so that you can tune the parameters to your liking.

[#download]
=== Download the model

With the InstructLab environment configured, you will now download a quantized (compressed and optimized) model to your local directory, to be used as a model server for API requests, or to help train a new model.

NOTE: We are using a quantized model because we are only leveraging a single GPU for this lab. For better performance or production use cases, you would use unquantized models.

Run the `ilab model download` command as shown below:

[source,sh,role=execute,subs=attributes+]
----
ilab model download --repository instructlab/granite-7b-lab-GGUF --filename=granite-7b-lab-Q4_K_M.gguf
----

The `ilab model download` command downloads a model from the HuggingFace Instructlab organization that we will use for this workshop. The output should look like the following:

[subs=quotes]
----
Downloading model from Hugging Face: instructlab/granite-7b-lab-GGUF@main to /home/instruct/.cache/instructlab/models...
Downloading 'granite-7b-lab-Q4_K_M.gguf' to '/home/instruct/.cache/instructlab/models/.cache/huggingface/download/granite-7b-lab-Q4_K_M.gguf.6adeaad8c048b35ea54562c55e454cc32c63118a32c7b8152cf706b290611487.incomplete'
INFO 2024-09-10 16:51:32,740 huggingface_hub.file_download:1908: Downloading 'granite-7b-lab-Q4_K_M.gguf' to '/home/instruct/.cache/instructlab/models/.cache/huggingface/download/granite-7b-lab-Q4_K_M.gguf.6adeaad8c048b35ea54562c55e454cc32c63118a32c7b8152cf706b290611487.incomplete'
granite-7b-lab-Q4_K_M.gguf: 100%|█| 4.08G/4.08G [00:19<00:00, 207
Download complete. Moving file to /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
INFO 2024-09-10 16:51:52,562 huggingface_hub.file_download:1924: Download complete. Moving file to /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----

Now that the model has been downloaded, we can serve and chat with the model. Serving the model simply means we are going to run a server that will allow other programs to interact with the data similar to making an API call. 

[#serve]
=== Serving the model

Let's serve the model by running the following command:

[source,sh,role=execute,subs=attributes+]
----
ilab model serve --model-path /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----

As you can see, the serve command can take an optional `-–model-path` argument. In this case, we want to serve the Granite model. If no model path is provided, the default value from the config.yaml file will be used. 

Once the model is served and ready, you’ll see the following output:

[subs=quotes]
----
INFO 2024-09-10 18:12:09,459 instructlab.model.serve:145: Using model '/home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size.
INFO 2024-09-10 18:12:09,459 instructlab.model.serve:149: Serving model '/home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf' with llama-cpp
INFO 2024-09-10 18:12:16,023 instructlab.model.backends.llama_cpp:250: Replacing chat template:
 {% for message in messages %}
{% if message['role'] == 'user' %}
{{ '<|user|>
' + message['content'] }}
{% elif message['role'] == 'system' %}
{{ '<|system|>
' + message['content'] }}
{% elif message['role'] == 'assistant' %}
{{ '<|assistant|>
' + message['content'] + eos_token }}
{% endif %}
{% if loop.last and add_generation_prompt %}
{{ '<|assistant|>' }}
{% endif %}
{% endfor %}
INFO 2024-09-10 18:12:16,026 instructlab.model.backends.llama_cpp:193: Starting server process, press CTRL+C to shutdown server...
INFO 2024-09-10 18:12:16,026 instructlab.model.backends.llama_cpp:194: After application startup complete see http://127.0.0.1:8000/docs for API.

----

*WOOHOO!* You just served the model for the first time and are ready to test out your work so far by interacting with the LLM. We are going to accomplish this by chatting with the model.

[#chat]
=== Chat with the model

Because you’re serving the model in one terminal window, you will have to use a separate terminal window and re-activate your Python virtual environment to run the ilab chat command. 

. In the *bottom* terminal window, issue the following commands:

[source,sh,role=execute,subs=attributes+]
----
cd ~/instructlab
source venv/bin/activate
----
.Your prompt should now look like this
[source,sh]
----
(venv) [instruct@instructlab instructlab]$ 
----

[start=2]
. Now that the environment is sourced, you can begin a chat session with the ilab chat command:

[source,sh,role=execute,subs=attributes+]
----
ilab model chat -m /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----

You should see a chat prompt like the example below.

[source,sh]
----
╭───────────────────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ GRANITE-7B-LAB-Q4_K_M.GGUF (type /h for help)                                                                                                                            
╰───────────────────────────────────────────────────────────────────────────╯
>>> 
----

[start=3]
. At this point, you can interact with the model by asking it a question. Example:
What is OpenShift in 20 words or less?

[source,sh,role=execute,subs=attributes+]
----
What is OpenShift in 20 words or less?
----

Wait, wut? That was AWESOME!!!!! You now have your own local LLM running on this machine. That was pretty easy, huh?

[#integrating_instructlab]
== Integrating AI into an Insurance Application

The previous section showed you the basics of how to interact with InstructLab. Now let's take things a step further by using InstructLab with an example application. We will use RHEL AI to leverage the Granite LLM, add additional data in the form of knowledge and/or skills, train the model with new knowledge and enable it to answer questions effectively. This is done in the context of Parasol, a fictional company that processes insurance claims.

Parasol has a chatbot application infused with AI (the Granite model) to provide repair suggestions for claims submitted. This would allow Parasol to expedite processing of various claims on hold. But at the moment, the chatbot does not provide effective repair suggestions. Using historical claims data that contain different repairs performed under different conditions, we show how users can add this knowledge to the Granite model, train it on the additional knowledge and improve its recommendations.

[#using_parasol_application]
=== Using the Parasol Application

Let's start by taking a look at the current experience a claims agent has when interacting with the chatbot. 

. While you may currently be in the *Terminals* view, switch to *Parasol* (in the top bar above the upper terminal window) to see the Parasol company's claims application in your browser.

image::parasol-view.png[]

As a claims agent, you can navigate and view the existing claims by clicking on the claim number on the screen. 

[start=2]
. For this lab we will be investigating *CLM195501* which is a claim that has been filed by Marty McFly, let's click on this claim now.

image::parasol-claim.png[]

You can read the details of the claim on this page and even expand the image of the DeLorean to check out how badly Marty wrecked his ride (note the flux capacitor on the ground). 

[start=3]
. Once you read the claim you click on the chatbot using the small blue icon in the bottom right of the page.

image::parasol-chat.webp[width=350]

IMPORTANT: This chatbot is backed by the Granite model you served earlier, so if you killed that running process you will need to restart it in your terminal by running the following: `ilab model serve --model-path /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf`

Let's imagine as a claims agent you'd like to know how much it might cost to repair a flux capacitor on Marty's DeLorean. 

[start=4]
. Ask the chatbot the following question:

[source,sh,role=execute,subs=attributes+]
----
How much does it cost to repair a flux capacitor?
----

You should see something similar to the following. Note that LLMs by nature are non-deterministic. This means that even with the same prompt input, the model will produce varying responses. So, your results may vary slightly.
You should see something similar to the following. Note that LLMs by nature are non-deterministic. This means that even with the same prompt input, the model will produce varying responses. So, your results may vary slightly.

image::parasol-chat-response.webp[width=350]

What we've already started to do is provide contextual information about the claim in each conversation with the LLM using Prompt Engineering. But unfortunately, the chatbot doesn't know how much it costs to repair a flux capacitor, nor will it have any domain-specific knowledge for our organization. With InstructLab and RHEL AI, we can change that by teaching the model!


[#taxononmy]
=== Understanding the Taxonomy

InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models (LLMs.) The "lab" in InstructLab stands for **L**arge-scale **A**lignment for Chat **B**ots. The LAB method is driven by taxonomies, which are largely created manually and with care.

InstructLab crowdsources the process of tuning and improving models by collecting two types of data: knowledge and skills, in the new InstructLab open source community. These submissions are collected in a taxonomy of YAML files to be used in the synthetic data generation process. To help you understand the directory structure of a taxonomy, please refer to the following image.

image::taxonomy.png[]

We are now going to leverage the taxonomy model to teach the model knowledge about a specific vehicle we cover and its details, from our organization's collection of public (and private) internal data.


Navigate back to the *Terminals* view. In the terminal window where you are running chat, enter `exit` to quit the chat session.

. Navigate to the taxonomy directory.

[source,sh,role=execute,subs=attributes+]
----
cd /home/instruct/.local/share/instructlab
tree taxonomy | head -n 10
----

.You should see the taxonomy directory listed as shown below:
[source,texinfo]
----
taxonomy
├── CODE_OF_CONDUCT.md
├── compositional_skills
│   ├── arts
│   ├── engineering
│   ├── general
│   │   └── synonyms
│   │       ├── attribution.txt
│   │       └── qna.yaml
│   ├── geography
----

Now, we need to create a directory where we can place our files.

[start=2]
. Create a directory to add new knowledge, demonstrating how to properly use the taxonomy structure to add knowledge with InstructLab. 

[source,sh,role=execute,subs=attributes+]
----
mkdir -p /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims
----

[start=3]
. Add new capabilities to our model through new knowledge. 

The way the taxonomy approach works is that we provide a file, named `qna.yaml`, that contains a sample data set of questions and answers. This data set will be used in the process of creating many more synthetic data examples, enough to fully influence the model's output. The important thing to understand about the `qna.yaml` file is that it must follow a specific schema for InstructLab to use it to synthetically generate more examples. 

The `qna.yaml` file is placed in a folder within the `knowledge` subdirectory of the taxonomy directory. It is placed in a folder with an appropriate name that is aligned with the data topic, as you will see in the below command.

[start=4]
. Instead of having to type a bunch of information in by hand, simply run the following command to copy this example https://raw.githubusercontent.com/rhai-code/backToTheFuture/main/qna.yaml[`qna.yaml`] file to your taxonomy directory:

[source,sh,role=execute,subs=attributes+]
----
cp -av ~/files/qna.yaml /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims/
----

[start=5]
. You can then verify the file was correctly copied by issuing the following command which will display the first 10 lines of the file:

[source,sh,role=execute,subs=attributes+]
----
head /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims/qna.yaml
----

During this workshop, we don’t expect you to type all of this information in by hand - we are including the content here for your reference.

It's a YAML file that consists of a list of Q&A examples that will be used by the trainer model to teach the student model. There is also a source document which is a link to a specific commit of a text file in git, where https://github.com/rhai-code/backToTheFuture/blob/main/data.md[we've included] that a flux capacitor costs an affordable $10,000,000. 

To help you understand the qna file format, we have included an excerpt of the file below. Feel free to view the entire file on the system with the following command:

[source,sh,role=execute,subs=attributes+]
----
cat /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims/qna.yaml
----

[source,yaml]
----
version: 3
domain: time_travel
created_by: Marty McFly
seed_examples:
  - context: |
      The DeLorean DMC-12 is a sports car manufactured by John DeLorean's DeLorean Motor Company
      for the American market from 1981 to 1983. The car features gull-wing doors and a stainless-steel body.
      It gained fame for its appearance as the time machine in the "Back to the Future" film trilogy.
    questions_and_answers:
      - question: |
          When was the DeLorean manufactured?
        answer: |
          The DeLorean was manufactured from 1981 to 1983.
      - question: |
          Who manufactured the DeLorean DMC-12?
        answer: |
          The DeLorean Motor Company manufactured the DeLorean DMC-12.
      - question: |
          What type of doors does the DeLorean DMC-12 have?
        answer: |
          Gull-wing doors.
document_outline: |
  Details and repair costs on a DeLorean DMC-12 car.
document:
  repo: https://github.com/gshipley/backToTheFuture.git
  commit: 8bd9220c616afe24b9673d94ec1adce85320809c
  patterns:<6>
    - data.md
----

<1> `domain`: Category of the knowledge
<2> `created_by`: The author of the contribution, typically a GitHub username
<3> `seed_examples`: A collection of 5 key/value entries. Each entry includes a `context`, which is a chunk of information from your markdown knowledge document, and at least 3 question and answer pairs related to the context block.
<4> `document_outline`: Provides an overview of the document you are submitting.
<5> `document`: The source of your knowledge contribution, consisting of a `repo` URL pointing to the knowlege markdown files, and `commit` SHA that contains the specific files
<6> `patterns`: A list of glob patterns specifying the markdown file(s) in your repository.


Now, it's time to verify that the seed data is curated properly.

[start=6]
. Validate your taxonomy

InstructLab allows you to validate your taxonomy files before generating additional data. You can accomplish this by using the `ilab taxonomy diff` command as shown below:

NOTE: Make sure you are still in the virtual environment indicated by the (venv) on the command line. If not, source the venv/bin/activate file again.

[source,sh,role=execute,subs=attributes+]
----
ilab taxonomy diff
----

.You should see the following output:
[source,sh]
----
knowledge/parasol/claims/qna.yaml
Taxonomy in /home/instruct/.local/share/instructlab/taxonomy is valid :)
----

[#synthetic_data]
=== Generating Synthetic Data

Okay, so far so good. Now, let’s move on to the AWESOME part. We are going to use our taxonomy, which contains our `qna.yaml` file, to have the LLM automatically generate more examples. The generate step can often take a while and is dependent on the number of instructions that you want to generate. In other words, this means that InstructLab will generate X number of additional questions and answers based on the samples provided. To give you an idea of how long this takes, generating 100 additional questions and answers typically takes about 7 minutes when using a nicely specced consumer-grade GPU-accelerated Linux machine. This can take around 15 minutes using Apple Silicon and depends on many factors. For the purpose of this workshop, we are only going to generate a small amount of additional samples to give you a sense of how it works. To do this, issue the following commands:

// . We will now run the command to generate data in the current terminal. The other terminal should still be serving the Granite model from earlier. If the model is no longer being served, serve the model again with the following command (in the other terminal window):

// [source,sh,role=execute,subs=attributes+]
// ----
// ilab model serve --model-path /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
// ----

// Now run the following command in the terminal window that is not serving the model:

[source,sh,role=execute,subs=attributes+]
----
ilab data generate --model /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf --sdg-scale-factor 5 --gpus 1
----

After running this command, you should see InstructLab is now synthetically generating examples based on the seed data you provided in the qna.yaml file. You will see output on your screen indicating the data is being generated as shown below:

[source,sh]
----
INFO 2024-09-10 20:00:59,404 numexpr.utils:148: Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
INFO 2024-09-10 20:00:59,404 numexpr.utils:161: NumExpr defaulting to 16 threads.
INFO 2024-09-10 20:00:59,675 datasets:59: PyTorch version 2.3.1 available.
INFO 2024-09-10 20:01:00,546 instructlab.model.backends.llama_cpp:104: Trying to connect to model server at http://127.0.0.1:8000/v1
WARNING 2024-09-10 20:01:00,555 instructlab.data.generate:291: Disabling SDG batching - unsupported with llama.cpp serving
Generating synthetic data using 'simple' pipeline, '/home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf' model, '/home/instruct/.local/share/instructlab/taxonomy' taxonomy, against http://127.0.0.1:8000/v1 server
INFO 2024-09-10 20:01:00,870 instructlab.sdg:375: Synthesizing new instructions. If you aren't satisfied with the generated instructions, interrupt training (Ctrl-C) and try adjusting your YAML files. Adding more examples may help.
----

Now that we have generated additional data, we can use the ilab train command to incorporate this data set with the model.

If you would like to view the data generated, the SDG process creates a jsonl file located in the `/home/instruct/.local/share/instructlab/datasets` directory named knowledge_train_msgs[TIMESTAMP].jsonl

NOTE: Using a scale factor of 5 is generally not enough synthetic data to effectively impact the knowledge or skill of a model. However, due to time constraints of this workshop, the goal is to simply show you how this works using real commands. You would typically want to use a scale factor of 30 which is the default value to train the model effectively.

Once the new data has been generated, the next step is to train the model with the updated knowledge. This is performed with the ilab train command.

NOTE: Training using the newly generated data is a time and resource intensive task. Depending on the number of epochs desired, internet connection for safetensor downloading, and other factors, it can take from 20 minutes up to an hour and is highly dependent on the hardware used. It is not required to train the model to continue with the lab as we will use an already trained model.

[#training]
=== Training and Interacting with the Model

Due to the time constraints of this lab, we will not actually be training the model. A trained model will be provided for you. However, to illustrate how training works, you would issue an `ilab model train` command with various parameters. 

// If we were training the model during this workshop, you would see the following output: 

// [source,sh]
// ----
// LINUX_TRAIN.PY: NUM EPOCHS IS:  1
// LINUX_TRAIN.PY: TRAIN FILE IS:  taxonomy_data/train_gen.jsonl
// LINUX_TRAIN.PY: TEST FILE IS:  taxonomy_data/test_gen.jsonl
// LINUX_TRAIN.PY: Using device 'cuda:0'
//   NVidia CUDA version: 12.1
//   AMD ROCm HIP version: n/a
//   cuda:0 is 'NVIDIA A10G' (15.3 GiB of 22.1 GiB free, capability: 8.6)
//   WARNING: You have less than 18253611008 GiB of free GPU memory on '{index}'. Training may fail, use slow shared host memory, or move some layers to CPU.
//   Training does not use the local InstructLab serve. Consider stopping the server to free up about 5 GiB of GPU memory.
// LINUX_TRAIN.PY: LOADING DATASETS
// Generating train split: 5 examples [00:00, 265.43 examples/s]
// Generating train split: 7 examples [00:00, 6582.99 examples/s]
// /home/instruct/instructlab/venv/lib64/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
//   warnings.warn(
// Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
// LINUX_TRAIN.PY: NOT USING 4-bit quantization
// LINUX_TRAIN.PY: LOADING THE BASE MODEL
// Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  5.95it/s]
// ----

. Since this process will take over an hour to complete we have provided a model that has already been through this process. First, let's stop the current model server in the *upper* window, with kbd:[CTRL+C] on the keyboard. In order to serve the newly trained model you can now run the following in the *upper* command window:

[source,sh,role=execute,subs=attributes+]
----
ilab model serve --model-family merlinite --model-path /home/instruct/summit-connect-merlinite-lab-Q4.gguf
----

It may take some seconds to start, but you should see the following:

[source,sh]
----
INFO 2024-08-06 17:04:12,748 serve.py:51: serve Using model 'models/summit-connect-merlinite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size.
INFO 2024-08-06 17:04:15,452 server.py:218: server Starting server process, press CTRL+C to shutdown server...
INFO 2024-08-06 17:04:15,452 server.py:219: server After application startup complete see http://127.0.0.1:8000/docs for API.
----

[#verify]
=== Verifying the Application

Now for the moment of truth. You’ve added knowledge, generated synthetic data, and retrained the model. Refresh your browser window where you were viewing Marty McFly’s claim in the Parasol insurance application

image::parasol-view.png[]

Click on the blue chatbot icon in the bottom right corner of the screen to open the chatbot.

image::parasol-chat.webp[width=350]

. Let’s ask the chatbot the same question with the newly trained model and see if the response has improved.

[source,sh,role=execute,subs=attributes+]
----
How much does it cost to repair a flux capacitor?
----

You should see something similar to the following (keep in mind that your output may look different due to the nature of large language models):

image::parasol-new-response.webp[width=350]

CONGRATULATIONS! You just trained a chatbot for Parasol insurance and will make every claims agent’s life a little better! 

[#conclusion]
== Conclusion

Woohoo young padawan, mission accomplished. Breathe in for a bit. We’re proud of you, and I dare say you’re an AI Engineer now. You’re probably wondering what the next steps are, so let me give you some suggestions.

Start playing with both skill and knowledge additions. This is to give something "new" to the model. You give it a chunk of data, something it doesn’t know about, and then train it on that. How could InstructLab-trained models help at your company? Which friend will you brag to first?

As you can see, InstructLab is pretty straightforward and most of the time you spend will be on curating new taxonomy content. Again, we’re so happy you made it this far, and remember if you have questions we are here to help, and are excited to see what you come up with!

Please visit the official project github at https://github.com/instructlab[www.github.com/instructlab] and check out the community repo to learn about how to get involved with the upstream community! Also, https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/ai[learn more about RHEL AI here] (which includes support for InstructLab, idemification for model output for the included Granite large language models, and a platform to run AI your own way on the hybrid cloud).
